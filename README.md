# AgentKit

A modern multi-agent AI assistant built with **LangGraph** for orchestration and **AG-UI** for real-time agent-frontend communication.

## ğŸš€ Features

- **Multi-Agent Orchestration**: Powered by LangGraph for complex workflow management
- **Real-time Streaming**: AG-UI protocol for live agent communication
- **Modern UI**: NextJS + Shadcn UI for a beautiful, responsive interface
- **Flexible LLM Integration**: Default Ollama support, extensible to OpenAI, Anthropic, and more
- **Thread Management**: Create and manage multiple conversation threads
- **Type-Safe**: Full TypeScript support on frontend, Python type hints on backend
- **Observability**: Optional LangFuse integration for monitoring and debugging
- **Local Storage**: Client-side persistence for threads and messages

## ğŸ“‹ Prerequisites

- **Python 3.10+** (for backend)
- **Node.js 18+** (for frontend)
- **Ollama** (for local LLM inference)

## ğŸ—ï¸ Architecture

### Backend
- **Framework**: FastAPI with streaming SSE endpoints
- **Agent Orchestration**: LangGraph multi-agent workflows
- **State Management**: LangGraph state graphs with conditional routing
- **LLM Provider**: Ollama (`qwen:7b` model by default)
- **Protocol**: AG-UI for real-time event streaming
- **Observability**: LangFuse integration (optional)

### Frontend
- **Framework**: NextJS 14 with App Router
- **UI Library**: Shadcn UI + Tailwind CSS
- **Type Safety**: Full TypeScript implementation
- **State Management**: React hooks
- **Protocol**: AG-UI client for event stream handling
- **Storage**: LocalStorage for thread persistence

### Communication Flow

```
User Input â†’ Frontend (AG-UI Client) â†’ Backend (FastAPI) â†’ LangGraph Agent
                                                                    â†“
                                                                LLM (Ollama)
                                                                    â†“
User Display â† Frontend â† AG-UI Events (SSE Stream) â† Backend â† Response
```

## ğŸ› ï¸ Quick Start

### 1. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment variables (optional)
cp .env.example .env
# Edit .env with your settings

# Ensure Ollama is running
ollama serve
ollama pull qwen:7b

# Start backend server
python -m backend.main
```

Backend will be available at `http://localhost:8000`

### 2. Frontend Setup

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Set up environment variables
cp .env.local.example .env.local
# Edit .env.local and set NEXT_PUBLIC_API_URL=http://localhost:8000

# Start development server
npm run dev
```

Frontend will be available at `http://localhost:3000`

## ğŸ“ Project Structure

```
agenkit/
â”œâ”€â”€ backend/                 # Python backend
â”‚   â”œâ”€â”€ agents/             # Agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py   # Base agent class with AG-UI integration
â”‚   â”‚   â””â”€â”€ chat_agent.py   # Chat agent implementation
â”‚   â”œâ”€â”€ api/                # API layer
â”‚   â”‚   â”œâ”€â”€ models.py       # Pydantic models
â”‚   â”‚   â””â”€â”€ routes.py       # FastAPI endpoints
â”‚   â”œâ”€â”€ graphs/             # LangGraph workflows
â”‚   â”‚   â””â”€â”€ chat_graph.py   # Chat workflow definition
â”‚   â”œâ”€â”€ llm/                # LLM provider integrations
â”‚   â”‚   â”œâ”€â”€ ollama_provider.py
â”‚   â”‚   â””â”€â”€ provider_factory.py
â”‚   â”œâ”€â”€ protocols/          # AG-UI protocol implementation
â”‚   â”‚   â”œâ”€â”€ event_encoder.py
â”‚   â”‚   â””â”€â”€ event_types.py
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â””â”€â”€ main.py             # FastAPI application entry
â”‚
â”œâ”€â”€ frontend/               # NextJS frontend
â”‚   â”œâ”€â”€ app/               # NextJS app directory
â”‚   â”‚   â”œâ”€â”€ globals.css    # Global styles
â”‚   â”‚   â”œâ”€â”€ layout.tsx     # Root layout
â”‚   â”‚   â””â”€â”€ page.tsx       # Home page
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ ChatContainer.tsx
â”‚   â”‚   â”œâ”€â”€ ChatHistory.tsx
â”‚   â”‚   â”œâ”€â”€ ChatInput.tsx
â”‚   â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”‚   â”œâ”€â”€ Layout.tsx
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â””â”€â”€ ui/           # Shadcn UI components
â”‚   â”œâ”€â”€ hooks/            # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ useAGUI.ts    # AG-UI integration hook
â”‚   â”‚   â”œâ”€â”€ useChatThreads.ts
â”‚   â”‚   â””â”€â”€ useMessages.ts
â”‚   â”œâ”€â”€ services/         # API and storage services
â”‚   â”‚   â”œâ”€â”€ agui-client.ts
â”‚   â”‚   â”œâ”€â”€ api.ts
â”‚   â”‚   â””â”€â”€ storage.ts
â”‚   â””â”€â”€ types/            # TypeScript type definitions
â”‚       â”œâ”€â”€ agent.ts
â”‚       â”œâ”€â”€ agui.ts
â”‚       â””â”€â”€ chat.ts
â”‚
â””â”€â”€ agents.md             # Development guidelines
```

## ğŸ”Œ API Reference

### Backend Endpoints

#### POST /api/chat
Chat with the agent using AG-UI protocol.

**Request:**
```json
{
  "messages": [
    {"role": "user", "content": "Hello, how can you help me?"}
  ]
}
```

**Response:** Server-Sent Events (SSE) stream with AG-UI events:
- `RUN_STARTED`: Agent execution begins
- `TEXT_MESSAGE_CHUNK`: Streaming text response
- `RUN_FINISHED`: Agent execution complete
- `ERROR`: Error occurred during execution

**Example:**
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{"messages":[{"role":"user","content":"Hello!"}]}'
```

#### GET /health
Health check endpoint.

**Response:**
```json
{
  "status": "healthy"
}
```

## ğŸ§© Key Components

### Backend

- **BaseAgent**: Abstract base class for agents with AG-UI event emission
- **ChatAgent**: Main conversational agent implementation
- **ChatGraph**: LangGraph workflow for chat orchestration
- **LLMProviderFactory**: Factory for creating LLM provider instances
- **AG-UI Event Encoder**: Custom JSON encoder for AG-UI events

### Frontend

- **useAGUI**: React hook for AG-UI event stream handling
- **useChatThreads**: Hook for thread management
- **useMessages**: Hook for message state management
- **ChatContainer**: Main chat interface component
- **MessageBubble**: Individual message display component
- **AG-UI Client**: Service for SSE connection management

## ğŸ”§ Configuration

### Backend Environment Variables

Create a `.env` file in the `backend/` directory:

```bash
# LLM Provider Settings
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen:7b

# Optional: LangFuse Observability
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com
```

### Frontend Environment Variables

Create a `.env.local` file in the `frontend/` directory:

```bash
# Backend API URL
NEXT_PUBLIC_API_URL=http://localhost:8000
```

## ğŸ§ª Development

### Adding New Agents

1. Create a new agent class in `backend/agents/` inheriting from `BaseAgent`
2. Implement the `_execute()` method with your agent logic
3. Emit AG-UI events for frontend visibility
4. Register the agent in the LangGraph workflow

Example:
```python
from agents.base_agent import BaseAgent

class MyCustomAgent(BaseAgent):
    async def _execute(self, state: dict) -> dict:
        await self.emit_thinking("Processing your request...")
        
        # Your agent logic here
        result = await self.process(state)
        
        await self.emit_complete("Task completed!")
        return {"result": result}
```

### Adding New UI Components

1. Create component in `frontend/components/`
2. Use Shadcn UI primitives from `components/ui/`
3. Integrate with AG-UI hooks for real-time updates
4. Add TypeScript types in `types/`

### Testing

**Backend:**
```bash
cd backend
pytest tests/
```

**Frontend:**
```bash
cd frontend
npm test
```

## ğŸ“š Documentation

Detailed documentation available in the `.docs/` directory:

- **Implementation Plans**: `.docs/1-implementation-plans/`
- **Knowledge Base**: `.docs/2-knowledge-base/`
- **Backend Guide**: [backend/README.md](backend/README.md)
- **Frontend Guide**: [frontend/README.md](frontend/README.md)
- **Development Guidelines**: [agents.md](agents.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **LangGraph** for multi-agent orchestration
- **AG-UI Protocol** for agent-frontend communication
- **Shadcn UI** for beautiful UI components
- **Ollama** for local LLM inference
- **FastAPI** for the robust backend framework
- **NextJS** for the modern frontend framework

## ğŸ› Troubleshooting

### Backend Issues

**Ollama connection fails:**
```bash
# Ensure Ollama is running
ollama serve

# Verify model is available
ollama list
ollama pull qwen:7b
```

**Port 8000 already in use:**
```bash
# Find and kill the process
lsof -ti:8000 | xargs kill -9
```

### Frontend Issues

**API connection fails:**
- Verify backend is running on `http://localhost:8000`
- Check `NEXT_PUBLIC_API_URL` in `.env.local`
- Check CORS settings in backend

**Build errors:**
```bash
# Clean install
rm -rf node_modules package-lock.json
npm install
```

## ğŸ“ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check existing documentation in `.docs/`
- Review [agents.md](agents.md) for development patterns

---

Built with â¤ï¸ using LangGraph, AG-UI, FastAPI, and NextJS
